{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Objects Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of objects (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Objects* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Path to COCO trained weights\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from objects import *\n",
    "\n",
    "config = ObjectsConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the objects dataset, `load_objects()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ObjectsDataset()\n",
    "dataset_train.load_objects(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ObjectsDataset()\n",
    "dataset_val.load_objects(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4ac0ad3cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRNwE6Dh9SgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4ac08b0208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4ac0a17dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy4AAACnCAYAAAD35AgmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACMJ1wAAIDxhAsAADDePwHMU93fT1KF4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4ac05e5d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.002\n",
      "\n",
      "Checkpoint Path: /home/orestis/repositories/Mask_RCNN/logs/objects20171106T0032/mask_rcnn_objects_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orestis/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/orestis/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:1987: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  7/100 [=>............................] - ETA: 3958s - loss: 4.5598 - rpn_class_loss: 0.0427 - rpn_bbox_loss: 1.6794 - mrcnn_class_loss: 0.4377 - mrcnn_bbox_loss: 1.3973 - mrcnn_mask_loss: 0.6639"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_objects.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InferenceConfig(ObjectsConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox[:,:4], gt_mask, gt_bbox[:,4], \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox[:,:4], gt_bbox[:,4],\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
