{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "parentPath = os.path.abspath(\"../..\")\n",
    "if parentPath not in sys.path:\n",
    "    sys.path.insert(0, parentPath)\n",
    "\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = parentPath\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "SCENENN_DIR = \"/home/orestisz/data/sceneNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import sceneNN\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Validation dataset\n",
    "dataset_val = ObjectsDataset()\n",
    "dataset_val.load_sceneNN(SCENENN_DIR, \"validation\")\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class InferenceConfig(ObjectsConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=inference_config, model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "print(dataset_val.image_info[image_id]['path'])\n",
    "image = original_image[:, :, 0:3]\n",
    "depth = original_image[:, :, 3]\n",
    "log(\"image\", image)\n",
    "log(\"depth\", depth)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "if 1:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    skip = 20\n",
    "    for x in range(image.shape[0]):\n",
    "        if x % skip == 0:\n",
    "            for y in range(image.shape[1]):\n",
    "                if y % skip == 0:\n",
    "#                     print(image[x, y] / 255.0)\n",
    "                    ax.scatter(\n",
    "                        x, y, depth[x, y],\n",
    "                        c=image[x, y] / 255.0)\n",
    "    plt.show()\n",
    "\n",
    "_, class_ids = dataset_val.load_mask(image_id)\n",
    "\n",
    "visualize.display_top_masks(image, gt_mask, class_ids, dataset_val.class_names, limit=1)\n",
    "\n",
    "visualize.display_instances(image, gt_bbox[:,:4], gt_mask, gt_bbox[:,4], \n",
    "                            dataset_val.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# model_paths = model.find_all()\n",
    "# for model_path in model_paths:\n",
    "#     print(\"Loading weights from \", model_path[0])\n",
    "#     for checkpoint in model_path[1]:\n",
    "#         print(\"    \" + checkpoint)\n",
    "model.load_weights(checkpoint, by_name=True)\n",
    "model.load_weights(model.find_last()[1], by_name=True)\n",
    "start = time.clock()\n",
    "r = model.detect([original_image])[0]\n",
    "print(time.clock() - start)\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                    dataset_val.class_names, r['scores'], title=checkpoint, ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox[:,:4], gt_bbox[:,4],\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
